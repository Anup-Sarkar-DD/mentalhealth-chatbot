{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hYfcekVpcK86"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3OMnb4XDhFtF"
      },
      "outputs": [],
      "source": [
        "# Paths (Colab default paths where files should be uploaded)\n",
        "model_dir = '/content/'\n",
        "val_csv_path = '/content/mentalchat16k_validation.csv'   # Ensure this matches your uploaded filename!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j1QWbEhhSg_",
        "outputId": "27b92b72-8274-476d-ea78-66bfd87c3fee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded on GPU (T4)\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer and fine-tuned model\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained(model_dir)\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained(model_dir)\n",
        "\n",
        "# Move model to GPU â€” Colab will use T4 GPU if enabled in runtime settings\n",
        "model = model.to(\"cuda\")\n",
        "model.eval()\n",
        "print(\"Model loaded on GPU (T4)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdop9C9-ic_R",
        "outputId": "325d9380-0d96-4276-f6a8-ac986c093126"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation dataset size: 2397 samples\n"
          ]
        }
      ],
      "source": [
        "# Load validation data\n",
        "val_df = pd.read_csv(val_csv_path)\n",
        "print(f\"Validation dataset size: {len(val_df)} samples\")\n",
        "\n",
        "texts = val_df['text']\n",
        "targets = val_df['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RMuI1Vu6igUQ"
      },
      "outputs": [],
      "source": [
        "# Batch size for inference\n",
        "batch_size = 8\n",
        "predictions = []\n",
        "\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    batch_texts = list(texts[i:i+batch_size])\n",
        "    inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=64)\n",
        "    decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "    predictions.extend(decoded_preds)\n",
        "\n",
        "val_df['prediction'] = predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZnW42SwijO9",
        "outputId": "e1158725-3973-4e2a-a32c-7a571a08426b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exact Match Accuracy: 0.0000\n",
            "Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
            "Predictions saved to /content/mentalchat16k_val_predictions.csv\n",
            "                                                   text  \\\n",
            "636   you are a helpful mental health counselling as...   \n",
            "862   you are a helpful mental health counselling as...   \n",
            "1595  you are a helpful mental health counselling as...   \n",
            "312   you are a helpful mental health counselling as...   \n",
            "2117  you are a helpful mental health counselling as...   \n",
            "\n",
            "                                                 target  \\\n",
            "636   im glad that you feel a sense of relief and th...   \n",
            "862   its great that youve reached out for counselin...   \n",
            "1595  experiencing excessive worrying and fear that ...   \n",
            "312   in situations where you experience intense anx...   \n",
            "2117  your feelings are valid and its important to a...   \n",
            "\n",
            "                                             prediction  \n",
            "636    im glad to hear that you feel supported and t...  \n",
            "862    it sounds like youre going through a really t...  \n",
            "1595   it can be challenging when anxiety starts to ...  \n",
            "312    managing anxiety can be challenging but there...  \n",
            "2117   i can only imagine how challenging it must be...  \n"
          ]
        }
      ],
      "source": [
        "# Evaluation metrics\n",
        "y_true = [t.lower().strip() for t in targets]\n",
        "y_pred = [p.lower().strip() for p in predictions]\n",
        "\n",
        "exact_match = sum(t == p for t, p in zip(y_true, y_pred)) / len(y_true)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "print(f\"Exact Match Accuracy: {exact_match:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Save results\n",
        "val_df.to_csv('/content/mentalchat16k_val_predictions.csv', index=False)\n",
        "print(\"Predictions saved to /content/mentalchat16k_val_predictions.csv\")\n",
        "\n",
        "# Display a few sample results\n",
        "print(val_df.sample(5)[['text', 'target', 'prediction']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKFGiM4WxjhX",
        "outputId": "e3fa5cf3-b6a3-4aeb-b22e-d85f21537684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c91f4caa23fbeece3af1994471abe6c9688bb081e05a60b652c92ecc90a3b982\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Average ROUGE-L F1: 0.1328\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "rouge_scores = [scorer.score(str(tgt), str(pred))['rougeL'].fmeasure for tgt, pred in zip(targets, predictions)]\n",
        "avg_rougeL = sum(rouge_scores) / len(rouge_scores)\n",
        "print(f\"Average ROUGE-L F1: {avg_rougeL:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNdgEaNAzDbl",
        "outputId": "223cb3ea-0020-494d-b25b-7bc65fbf9558"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test dataset size: 2397 samples\n"
          ]
        }
      ],
      "source": [
        "# Load test dataset\n",
        "test_csv_path = '/content/mentalchat16k_test.csv'\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "print(f\"Test dataset size: {len(test_df)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3ySb8cBD1KWJ"
      },
      "outputs": [],
      "source": [
        "test_texts = test_df['text']\n",
        "test_targets = test_df['target']\n",
        "\n",
        "# Inference for test dataset using previously loaded tokenizer and model\n",
        "batch_size = 8\n",
        "test_predictions = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eSmaDxz41NIG"
      },
      "outputs": [],
      "source": [
        "for i in range(0, len(test_texts), batch_size):\n",
        "    batch_texts = list(test_texts[i:i + batch_size])\n",
        "    inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=64)\n",
        "    decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "    test_predictions.extend(decoded_preds)\n",
        "\n",
        "test_df['prediction'] = test_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpxm37Ym1Qvi",
        "outputId": "7b70a96f-f8af-4056-ade3-23c274a9d9e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Exact Match Accuracy: 0.0000\n",
            "Test Average ROUGE-L F1: 0.1323\n",
            "Test predictions saved to /content/mentalchat16k_test_predictions.csv\n",
            "                                                   text  \\\n",
            "1821  you are a helpful mental health counselling as...   \n",
            "1553  you are a helpful mental health counselling as...   \n",
            "722   you are a helpful mental health counselling as...   \n",
            "1819  you are a helpful mental health counselling as...   \n",
            "50    you are a helpful mental health counselling as...   \n",
            "\n",
            "                                                 target  \\\n",
            "1821  managing social anxiety can be challenging but...   \n",
            "1553  its great that you recognize the impact of str...   \n",
            "722   finding healthier ways to deal with stress is ...   \n",
            "1819  it sounds like youre dealing with a lot right ...   \n",
            "50    i can only imagine how difficult it must be fo...   \n",
            "\n",
            "                                             prediction  \n",
            "1821   it sounds like youve been experiencing a lot ...  \n",
            "1553   it sounds like stress has been overwhelming f...  \n",
            "722    it sounds like youre dealing with a lot right...  \n",
            "1819   it can be incredibly challenging to navigate ...  \n",
            "50     your feelings of guilt and helplessness are u...  \n"
          ]
        }
      ],
      "source": [
        "# Evaluation metrics for test set\n",
        "y_true_test = [t.lower().strip() for t in test_targets]\n",
        "y_pred_test = [p.lower().strip() for p in test_predictions]\n",
        "\n",
        "exact_match_test = sum(t == p for t, p in zip(y_true_test, y_pred_test)) / len(y_true_test)\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "rouge_scores_test = [scorer.score(t, p)['rougeL'].fmeasure for t, p in zip(test_targets, test_predictions)]\n",
        "avg_rougeL_test = sum(rouge_scores_test) / len(rouge_scores_test)\n",
        "\n",
        "print(f\"Test Exact Match Accuracy: {exact_match_test:.4f}\")\n",
        "print(f\"Test Average ROUGE-L F1: {avg_rougeL_test:.4f}\")\n",
        "\n",
        "# Save test predictions\n",
        "test_df.to_csv('/content/mentalchat16k_test_predictions.csv', index=False)\n",
        "print(\"Test predictions saved to /content/mentalchat16k_test_predictions.csv\")\n",
        "\n",
        "print(test_df.sample(5)[['text', 'target', 'prediction']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvQ2Rvy6L744",
        "outputId": "8239cb9d-13d6-4516-f47f-34acddabf563"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
        "\n",
        "# model_dir = 'blenderbot_mentalhealth_finetuned'\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# tokenizer = BlenderbotTokenizer.from_pretrained(model_dir)\n",
        "# model = BlenderbotForConditionalGeneration.from_pretrained(model_dir).to(device)\n",
        "# model.eval()\n",
        "\n",
        "# print(f\"Model loaded on {device}\")\n",
        "\n",
        "# print(\"Start chatting with the bot! Type 'exit' to quit.\")\n",
        "\n",
        "# while True:\n",
        "#     user_input = input(\"You: \")\n",
        "#     if user_input.lower() == 'exit':\n",
        "#         print(\"Exiting chat.\")\n",
        "#         break\n",
        "#     inputs = tokenizer(user_input, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
        "#     with torch.no_grad():\n",
        "#         generated_ids = model.generate(\n",
        "#             **inputs,\n",
        "#             max_new_tokens=64,\n",
        "#             do_sample=True,\n",
        "#             top_k=50,\n",
        "#             top_p=0.95,\n",
        "#             temperature=0.8\n",
        "#         )\n",
        "#     response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "#     print(f\"Bot: {response}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "model/evaluate_val.ipynb\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "model_dir = './model/blenderbot_mentalhealth_finetuned'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained(model_dir)\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained(model_dir).to(device)\n",
        "model.eval()\n",
        "\n",
        "val_df = pd.read_csv('data/mentalchat16k_validation.csv')\n",
        "\n",
        "texts = val_df['text']\n",
        "targets = val_df['target']\n",
        "batch_size = 8\n",
        "predictions = []\n",
        "\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    batch_texts = list(texts[i:i+batch_size])\n",
        "    inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=64, do_sample=True, top_k=50, top_p=0.95, temperature=0.8)\n",
        "    decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "    predictions.extend(decoded_preds)\n",
        "\n",
        "val_df['prediction'] = predictions\n",
        "\n",
        "y_true = [t.lower().strip() for t in targets]\n",
        "y_pred = [p.lower().strip() for p in predictions]\n",
        "\n",
        "exact_match = sum(t == p for t, p in zip(y_true, y_pred)) / len(y_true)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "print(f\"Exact Match Accuracy: {exact_match:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "rouge_scores = [scorer.score(t, p)['rougeL'].fmeasure for t, p in zip(targets, predictions)]\n",
        "avg_rougeL = sum(rouge_scores) / len(rouge_scores)\n",
        "print(f\"Average ROUGE-L F1: {avg_rougeL:.4f}\")\n",
        "\n",
        "val_df.to_csv('data/mentalchat16k_val_predictions.csv', index=False)\n",
        "print(\"Validation predictions saved at data/mentalchat16k_val_predictions.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "model/evaluate_test.ipynb\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "model_dir = './model/blenderbot_mentalhealth_finetuned'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained(model_dir)\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained(model_dir).to(device)\n",
        "model.eval()\n",
        "\n",
        "test_df = pd.read_csv('data/mentalchat16k_test.csv')\n",
        "\n",
        "texts = test_df['text']\n",
        "targets = test_df['target']\n",
        "batch_size = 8\n",
        "predictions = []\n",
        "\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    batch_texts = list(texts[i:i+batch_size])\n",
        "    inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=64, do_sample=True, top_k=50, top_p=0.95, temperature=0.8)\n",
        "    decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "    predictions.extend(decoded_preds)\n",
        "\n",
        "test_df['prediction'] = predictions\n",
        "\n",
        "y_true = [t.lower().strip() for t in targets]\n",
        "y_pred = [p.lower().strip() for p in predictions]\n",
        "\n",
        "exact_match = sum(t == p for t, p in zip(y_true, y_pred)) / len(y_true)\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "rouge_scores = [scorer.score(t, p)['rougeL'].fmeasure for t, p in zip(targets, predictions)]\n",
        "avg_rougeL = sum(rouge_scores) / len(rouge_scores)\n",
        "\n",
        "print(f\"Test Exact Match Accuracy: {exact_match:.4f}\")\n",
        "print(f\"Test Average ROUGE-L F1: {avg_rougeL:.4f}\")\n",
        "\n",
        "test_df.to_csv('data/mentalchat16k_test_predictions.csv', index=False)\n",
        "print(\"Test set predictions saved at data/mentalchat16k_test_predictions.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
